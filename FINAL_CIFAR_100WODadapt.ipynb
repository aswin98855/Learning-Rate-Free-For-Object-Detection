{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uz-1G6IqMui"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import pdb\n",
        "import logging\n",
        "import os\n",
        "import torch.distributed as dist\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from torch.optim.optimizer import _params_t\n",
        "else:\n",
        "    _params_t = Any\n",
        "\n",
        "class DAdaptAdam(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1.0,\n",
        "                 betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, log_every=0,\n",
        "                 decouple=False,\n",
        "                 use_bias_correction=False,\n",
        "                 d0=1e-6, growth_rate=float('inf'),\n",
        "                 fsdp_in_use=False):\n",
        "        if not 0.0 < d0:\n",
        "            raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
        "        if not 0.0 < lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 < eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "        if decouple:\n",
        "            print(f\"Using decoupled weight decay\")\n",
        "\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay,\n",
        "                        d = d0,\n",
        "                        k=0,\n",
        "                        layer_scale=1.0,\n",
        "                        numerator_weighted=0.0,\n",
        "                        log_every=log_every,\n",
        "                        growth_rate=growth_rate,\n",
        "                        use_bias_correction=use_bias_correction,\n",
        "                        decouple=decouple,\n",
        "                        fsdp_in_use=fsdp_in_use)\n",
        "        self.d0 = d0\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @property\n",
        "    def supports_memory_efficient_fp16(self):\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def supports_flat_params(self):\n",
        "        return True\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        sk_l1 = 0.0\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        use_bias_correction = group['use_bias_correction']\n",
        "        numerator_weighted = group['numerator_weighted']\n",
        "        beta1, beta2 = group['betas']\n",
        "        k = group['k']\n",
        "\n",
        "        d = group['d']\n",
        "        lr = max(group['lr'] for group in self.param_groups)\n",
        "\n",
        "        if use_bias_correction:\n",
        "            bias_correction = ((1-beta2**(k+1))**0.5)/(1-beta1**(k+1))\n",
        "        else:\n",
        "            bias_correction = 1\n",
        "\n",
        "        dlr = d*lr*bias_correction\n",
        "\n",
        "        growth_rate = group['growth_rate']\n",
        "        decouple = group['decouple']\n",
        "        log_every = group['log_every']\n",
        "        fsdp_in_use = group['fsdp_in_use']\n",
        "\n",
        "\n",
        "        sqrt_beta2 = beta2**(0.5)\n",
        "\n",
        "        numerator_acum = 0.0\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            decay = group['weight_decay']\n",
        "            k = group['k']\n",
        "            eps = group['eps']\n",
        "            group_lr = group['lr']\n",
        "            r = group['layer_scale']\n",
        "\n",
        "            if group_lr not in [lr, 0.0]:\n",
        "                raise RuntimeError(f\"Setting different lr values in different parameter groups \"\n",
        "                                   \"is only supported for values of 0. To scale the learning \"\n",
        "                                   \"rate differently for each layer, set the 'layer_scale' value instead.\")\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                if hasattr(p, \"_fsdp_flattened\"):\n",
        "                    fsdp_in_use = True\n",
        "\n",
        "                grad = p.grad.data\n",
        "\n",
        "                # Apply weight decay (coupled variant)\n",
        "                if decay != 0 and not decouple:\n",
        "                    grad.add_(p.data, alpha=decay)\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if 'step' not in state:\n",
        "                    state['step'] = 0\n",
        "                    state['s'] = torch.zeros_like(p.data).detach()\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "                s = state['s']\n",
        "\n",
        "                if group_lr > 0.0:\n",
        "                    denom = exp_avg_sq.sqrt().add_(eps)\n",
        "                    numerator_acum += r * dlr * torch.dot(grad.flatten(), s.div(denom).flatten()).item()\n",
        "\n",
        "                    # Adam EMA updates\n",
        "                    exp_avg.mul_(beta1).add_(grad, alpha=r*dlr*(1-beta1))\n",
        "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
        "\n",
        "                    s.mul_(sqrt_beta2).add_(grad, alpha=dlr*(1-sqrt_beta2))\n",
        "                    sk_l1 += r * s.abs().sum().item()\n",
        "\n",
        "            ######\n",
        "\n",
        "        numerator_weighted = sqrt_beta2*numerator_weighted + (1-sqrt_beta2)*numerator_acum\n",
        "        d_hat = d\n",
        "        if sk_l1 == 0:\n",
        "            return loss\n",
        "\n",
        "        if lr > 0.0:\n",
        "            if fsdp_in_use:\n",
        "                dist_tensor = torch.zeros(2).cuda()\n",
        "                dist_tensor[0] = numerator_weighted\n",
        "                dist_tensor[1] = sk_l1\n",
        "                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
        "                global_numerator_weighted = dist_tensor[0]\n",
        "                global_sk_l1 = dist_tensor[1]\n",
        "            else:\n",
        "                global_numerator_weighted = numerator_weighted\n",
        "                global_sk_l1 = sk_l1\n",
        "\n",
        "\n",
        "            d_hat = global_numerator_weighted/((1-sqrt_beta2)*global_sk_l1)\n",
        "            d = max(d, min(d_hat, d*growth_rate))\n",
        "\n",
        "        if log_every > 0 and k % log_every == 0:\n",
        "            logging.info(f\"lr: {lr} dlr: {dlr} d_hat: {d_hat}, d: {d}. sk_l1={global_sk_l1:1.1e} numerator_weighted={global_numerator_weighted:1.1e}\")\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            group['numerator_weighted'] = numerator_weighted\n",
        "            group['d'] = d\n",
        "\n",
        "            decay = group['weight_decay']\n",
        "            k = group['k']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(eps)\n",
        "\n",
        "                # Apply weight decay (decoupled variant)\n",
        "                if decay != 0 and decouple:\n",
        "                    p.data.add_(p.data, alpha=-decay * dlr)\n",
        "\n",
        "\n",
        "                ### Take step\n",
        "                p.data.addcdiv_(exp_avg, denom, value=-1)\n",
        "\n",
        "            group['k'] = k + 1\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define a more complex neural network model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = torch.nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = torch.nn.Linear(512, 256)\n",
        "        self.fc3 = torch.nn.Linear(256, 100)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "net = Net()\n",
        "\n",
        "# Define data transforms including data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use Adam optimizer\n",
        "#optimizer = DAdaptAdam(net.parameters(), lr=1)\n",
        "# Define a learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR by a factor of 0.1 every 10 epochs\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # Train for 20 epochs\n",
        "    net.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(trainloader):.3f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    scheduler.step()  # Update learning rate scheduler\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXdPSXoeqafH",
        "outputId": "a0a2ce78-8eb3-4334-a4bc-97f8620b0e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 4.255, Accuracy: 4.31%\n",
            "Accuracy on test set: 9.82%\n",
            "Epoch 2, Loss: 3.858, Accuracy: 9.24%\n",
            "Accuracy on test set: 15.28%\n",
            "Epoch 3, Loss: 3.638, Accuracy: 12.82%\n",
            "Accuracy on test set: 20.16%\n",
            "Accuracy on test set: 24.03%\n",
            "Epoch 5, Loss: 3.339, Accuracy: 18.21%\n",
            "Accuracy on test set: 25.62%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@ARTICLE{defazio2023dadapt,\n",
        "\n",
        "\n",
        "author = {Aaron Defazio and Konstantin Mishchenko},  \n",
        "\n",
        "\n",
        "title = {Learning-Rate-Free Learning by D-Adaptation},\n",
        "\n",
        "\n",
        "journal = {The 40th International Conference on Machine Learning (ICML 2023)},\n",
        "\n",
        "\n",
        "year = {2023}\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "gaNLULEheng7"
      }
    }
  ]
}