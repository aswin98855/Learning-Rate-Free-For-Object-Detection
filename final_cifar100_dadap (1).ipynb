{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiSFAL62btOb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import pdb\n",
        "import logging\n",
        "import os\n",
        "import torch.distributed as dist\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from torch.optim.optimizer import _params_t\n",
        "else:\n",
        "    _params_t = Any\n",
        "\n",
        "class DAdaptAdam(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1.0,\n",
        "                 betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, log_every=0,\n",
        "                 decouple=False,\n",
        "                 use_bias_correction=False,\n",
        "                 d0=1e-6, growth_rate=float('inf'),\n",
        "                 fsdp_in_use=False):\n",
        "        if not 0.0 < d0:\n",
        "            raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
        "        if not 0.0 < lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 < eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "        if decouple:\n",
        "            print(f\"Using decoupled weight decay\")\n",
        "\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay,\n",
        "                        d = d0,\n",
        "                        k=0,\n",
        "                        layer_scale=1.0,\n",
        "                        numerator_weighted=0.0,\n",
        "                        log_every=log_every,\n",
        "                        growth_rate=growth_rate,\n",
        "                        use_bias_correction=use_bias_correction,\n",
        "                        decouple=decouple,\n",
        "                        fsdp_in_use=fsdp_in_use)\n",
        "        self.d0 = d0\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @property\n",
        "    def supports_memory_efficient_fp16(self):\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def supports_flat_params(self):\n",
        "        return True\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        sk_l1 = 0.0\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        use_bias_correction = group['use_bias_correction']\n",
        "        numerator_weighted = group['numerator_weighted']\n",
        "        beta1, beta2 = group['betas']\n",
        "        k = group['k']\n",
        "\n",
        "        d = group['d']\n",
        "        lr = max(group['lr'] for group in self.param_groups)\n",
        "\n",
        "        if use_bias_correction:\n",
        "            bias_correction = ((1-beta2**(k+1))**0.5)/(1-beta1**(k+1))\n",
        "        else:\n",
        "            bias_correction = 1\n",
        "\n",
        "        dlr = d*lr*bias_correction\n",
        "\n",
        "        growth_rate = group['growth_rate']\n",
        "        decouple = group['decouple']\n",
        "        log_every = group['log_every']\n",
        "        fsdp_in_use = group['fsdp_in_use']\n",
        "\n",
        "\n",
        "        sqrt_beta2 = beta2**(0.5)\n",
        "\n",
        "        numerator_acum = 0.0\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            decay = group['weight_decay']\n",
        "            k = group['k']\n",
        "            eps = group['eps']\n",
        "            group_lr = group['lr']\n",
        "            r = group['layer_scale']\n",
        "\n",
        "            if group_lr not in [lr, 0.0]:\n",
        "                raise RuntimeError(f\"Setting different lr values in different parameter groups \"\n",
        "                                   \"is only supported for values of 0. To scale the learning \"\n",
        "                                   \"rate differently for each layer, set the 'layer_scale' value instead.\")\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                if hasattr(p, \"_fsdp_flattened\"):\n",
        "                    fsdp_in_use = True\n",
        "\n",
        "                grad = p.grad.data\n",
        "\n",
        "                # Apply weight decay (coupled variant)\n",
        "                if decay != 0 and not decouple:\n",
        "                    grad.add_(p.data, alpha=decay)\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if 'step' not in state:\n",
        "                    state['step'] = 0\n",
        "                    state['s'] = torch.zeros_like(p.data).detach()\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "                s = state['s']\n",
        "\n",
        "                if group_lr > 0.0:\n",
        "                    denom = exp_avg_sq.sqrt().add_(eps)\n",
        "                    numerator_acum += r * dlr * torch.dot(grad.flatten(), s.div(denom).flatten()).item()\n",
        "\n",
        "                    # Adam EMA updates\n",
        "                    exp_avg.mul_(beta1).add_(grad, alpha=r*dlr*(1-beta1))#\n",
        "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
        "\n",
        "                    s.mul_(sqrt_beta2).add_(grad, alpha=dlr*(1-sqrt_beta2))\n",
        "                    sk_l1 += r * s.abs().sum().item()\n",
        "\n",
        "            ######\n",
        "\n",
        "        numerator_weighted = sqrt_beta2*numerator_weighted + (1-sqrt_beta2)*numerator_acum\n",
        "        d_hat = d\n",
        "        if sk_l1 == 0:\n",
        "            return loss\n",
        "\n",
        "        if lr > 0.0:\n",
        "            if fsdp_in_use:\n",
        "                dist_tensor = torch.zeros(2).cuda()\n",
        "                dist_tensor[0] = numerator_weighted\n",
        "                dist_tensor[1] = sk_l1\n",
        "                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
        "                global_numerator_weighted = dist_tensor[0]\n",
        "                global_sk_l1 = dist_tensor[1]\n",
        "            else:\n",
        "                global_numerator_weighted = numerator_weighted\n",
        "                global_sk_l1 = sk_l1\n",
        "\n",
        "\n",
        "            d_hat = global_numerator_weighted/((1-sqrt_beta2)*global_sk_l1)\n",
        "            d = max(d, min(d_hat, d*growth_rate))\n",
        "\n",
        "        if log_every > 0 and k % log_every == 0:\n",
        "            logging.info(f\"lr: {lr} dlr: {dlr} d_hat: {d_hat}, d: {d}. sk_l1={global_sk_l1:1.1e} numerator_weighted={global_numerator_weighted:1.1e}\")\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            group['numerator_weighted'] = numerator_weighted\n",
        "            group['d'] = d\n",
        "\n",
        "            decay = group['weight_decay']\n",
        "            k = group['k']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(eps)\n",
        "\n",
        "                # Apply weight decay (decoupled variant)\n",
        "                if decay != 0 and decouple:\n",
        "                    p.data.add_(p.data, alpha=-decay * dlr)\n",
        "\n",
        "\n",
        "                ### Take step\n",
        "                p.data.addcdiv_(exp_avg, denom, value=-1)\n",
        "\n",
        "            group['k'] = k + 1\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # CIFAR-100 images are 32x32 RGB images\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128*4*4, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)  # Adjusted for CIFAR-100\n",
        "        self.fc3 = nn.Linear(256, 100)\n",
        "\n",
        "        self.dropout=nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load CIFAR-100 data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_data = datasets.CIFAR100(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR100(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the network and optimizer\n",
        "model = SimpleCNN()\n",
        "optimizer = DAdaptAdam(model.parameters(), lr=1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # Loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvYKzlQybvFJ",
        "outputId": "176b4727-d78f-44d1-c89c-849b95094d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 4.2089831076009805\n",
            "Epoch 2, Loss: 3.6109540386273125\n",
            "Epoch 3, Loss: 3.3119604727801155\n",
            "Epoch 4, Loss: 3.095112749072902\n",
            "Epoch 5, Loss: 2.9331651968724284\n",
            "Accuracy of the network on the test images: 27.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@ARTICLE{defazio2023dadapt,\n",
        "\n",
        "\n",
        "author = {Aaron Defazio and Konstantin Mishchenko},  \n",
        "\n",
        "\n",
        "title = {Learning-Rate-Free Learning by D-Adaptation},\n",
        "\n",
        "\n",
        "journal = {The 40th International Conference on Machine Learning (ICML 2023)},\n",
        "\n",
        "\n",
        "year = {2023}\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "emV6AcDVejr4"
      }
    }
  ]
}